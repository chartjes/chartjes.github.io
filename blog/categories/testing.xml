<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title><![CDATA[Grumpy Learning]]></title>
    <link href="https://grumpy-learning.com/blog/categories/testing.xml" rel="self"/>
    <link href="https://grumpy-learning.com/"/>
    <updated>2022-12-09T23:36:46+00:00</updated>
    <id>https://grumpy-learning.com/</id>
        <generator uri="http://sculpin.io/">Sculpin</generator>
            <entry>
            <title type="html"><![CDATA[Testing decoupled PHP code?]]></title>
            <link href="https://grumpy-learning.com/blog/2022/12/06/testing-decoupled-code/"/>
            <updated>2022-12-06T00:00:00+00:00</updated>
            <id>https://grumpy-learning.com/blog/2022/12/06/testing-decoupled-code/</id>
            <content type="html"><![CDATA[<p>One of the reasons many experienced developers encourage the concept of
"decoupling your code" is so that it makes testing your code straightforward.
I wanted to share an example of how I went about writing some tests for
some code that I had refactored from being a tangled spaghetti-like mess.</p>

<p>Here is the code I am looking at, written targeting PHP 8.1.</p>

<pre><code class="php">&lt;?php
declare(strict_types=1);

namespace Webreg\Query;

use Slim\Psr7\Response;
use Slim\Psr7\Request;
use Twig\Environment;
use Webreg\Repository\GameRepository;
use Webreg\ViewModel\Rotations;

final class RotationManagementQuery
{
    public function __construct(
        private Environment $twig,
        private GameRepository $gameRepository,
        private Rotations $rotations
    ) {}

    public function __invoke(Request $request): Response
    {
        $params = $request-&gt;getQueryParams();
        $maxWeek = $this-&gt;gameRepository-&gt;getMaxWeek();
        $week = (isset($params['week'])) ? (int) $params['week'] : $maxWeek;
        $rotations = $this-&gt;rotations-&gt;getAllByWeek($week);
        $response = new Response(200, null);
        $response-&gt;getBody()-&gt;write($this-&gt;twig-&gt;render('rotations/management.twig', [
            'current_week' =&gt; $week,
            'rotations' =&gt; $rotations,
        ]));

        return $response;
    }
}
</code></pre>

<p>I am using <a href="https://www.martinfowler.com/bliki/CQRS.html">Command Query Responsibility Segregation</a> in
this application's architecture, and this bit of code is a Query that will
retrieve a collection of pitchers who will be starting for baseball
teams in my simulation baseball league for a particular week.</p>

<p>In following some rules for decoupling, you can see some of the
following decisions had been made:</p>

<ul>
<li>not extending off of a base class</li>
<li>all dependencies are injected at run time</li>
<li>single-method for the class</li>
</ul>

<h2 id="identifying-dependencies">Identifying Dependencies</h2>

<p>So what are the dependencies I will need?</p>

<ul>
<li>a <a href="https://twig.symfony.com">Twig</a> object</li>
<li>a <a href="https://martinfowler.com/eaaCatalog/repository.html">repository</a> object for retrieving data</li>
<li>a <a href="https://martinfowler.com/eaaDev/PresentationModel.html">view model</a> for presenting the data</li>
<li>an object that contains the HTTP <a href="https://www.php-fig.org/psr/psr-7/">request</a></li>
</ul>

<p>In the old architecture, I was creating those dependencies deep inside the "business logic"
and therefore it was very hard to write anything other than some kind of "check
the HTML output" type of test. Ironically that is what the new test does as well but
this sort of decoupled architecture leads to a much more straightforward test.</p>

<h2 id="identifying-output">Identifying Output</h2>

<p>In these tests, I wanted to make sure that if I had at least one rotation
stored in the database for a particular week, when the page renders I should
see that rotation in the output somewhere.</p>

<h2 id="test-skeleton">Test Skeleton</h2>

<p>As always, I break out the <a href="http://wiki.c2.com/?ArrangeActAssert">Arrange-Act-Assert</a> pattern
to create the skeleton of the test:</p>

<pre><code class="php">    /** @test */
    public function it_returns_expected_rotation(): void
    {
        // Arrange

        // Act

        // Assert
        self::fail();
    }
</code></pre>

<p>Remember, you always want to start with a failing test.</p>

<h2 id="arranging-our-dependencies">Arranging our dependencies</h2>

<p>These days I try and use the fewest number of <a href="https://phpunit.readthedocs.io/en/9.5/test-doubles.html">test doubles</a>
in my test scenarios. Given the dependencies I needed,
I was going to need three "fake" dependencies, configured
to provide only the implementation details required to make
the scenario work.</p>

<p>I don't want to get into a longer discussion on the use of
test doubles except to say that the decoupling strategy
I am using will minimize the chances that any doubles drift
from how the code is actually implemented.</p>

<p>Trust me, I do this for a living!</p>

<pre><code class="php">    /** @test */
    public function it_returns_expected_rotation(): void
    {
        // Arrange
        $loader = new FilesystemLoader(__DIR__ . '/../../templates/');
        $twig = new Environment($loader);

        $gamesRepo = $this-&gt;createMock(GameRepository::class);
        $gamesRepo-&gt;expects($this-&gt;once())
            -&gt;method('getMaxWeek')
            -&gt;willReturn(1);

        $testRotation = new ArrayCollection();
        $testRotation-&gt;add([
            'franchise_id' =&gt; 1,
            'ibl' =&gt; 'MAD',
            'rotation' =&gt; 'One, Two, Three'
        ]);

        $viewModel = $this-&gt;createMock(RotationsUsingDoctrine::class);
        $viewModel-&gt;expects($this-&gt;once())
            -&gt;method('getAllByWeek')
            -&gt;willReturn($testRotation);

        $request = $this-&gt;createMock(Request::class);
        $request-&gt;expects($this-&gt;once())
            -&gt;method('getQueryParams')
            -&gt;willReturn(['week' =&gt; 1]);

        // Act

        // Assert
        self::fail();
    }
</code></pre>

<h2 id="acting-on-the-code-under-test">Acting on the code-under-test</h2>

<p>This has always struck me as a weird way to describe "executing the
code we are testing" but I guess "Arrange-Execute-Assert" doesn't
flow in English quite the same way.</p>

<p>Now that I have all my dependencies created and configured the way
I need, time to run the code and grab some results I can test.</p>

<pre><code class="php">    /** @test */
    public function it_returns_expected_rotation(): void
    {
        // Arrange

        // Act
        $query = new RotationManagementQuery($twig, $gamesRepo, $viewModel);
        $results = $query-&gt;__invoke($request);

        // Assert
        self::fail();
    }
</code></pre>

<h2 id="asserting-results-of-code-execution">Asserting results of code execution</h2>

<p>Just like I did before, I am checking the HTML output from executing
this Query to make sure I am seeing values that I expect</p>

<pre><code class="php">    /** @test */
    public function it_returns_expected_rotation(): void
    {
        // Arrange

        // Act

        // Assert
        self::assertStringContainsString('One, Two, Three', $results-&gt;getBody());
    }
</code></pre>

<p>When building my assertions, I tend to go with "what are the
fewest number of things I need to do in order to prove the
code is working as expected."</p>

<p>In this case, I felt checking that I see an expected "pitching rotation"
in the output is good enough.</p>

<p>Here is what the whole test looks like:</p>

<pre><code class="php">&lt;?php

namespace Webreg\Test\Query;

use Doctrine\Common\Collections\ArrayCollection;
use Slim\Psr7\Request;
use Twig\Environment;
use Twig\Loader\FilesystemLoader;
use Webreg\Query\RotationManagementQuery;
use PHPUnit\Framework\TestCase;
use Webreg\Repository\GameRepository;
use Webreg\ViewModel\RotationsUsingDoctrine;

class RotationManagementQueryTest extends TestCase
{
    /** @test */
    public function it_returns_expected_rotation(): void
    {
        // Arrange
        $loader = new FilesystemLoader(__DIR__ . '/../../templates/');
        $twig = new Environment($loader);

        $gamesRepo = $this-&gt;createMock(GameRepository::class);
        $gamesRepo-&gt;expects($this-&gt;once())
            -&gt;method('getMaxWeek')
            -&gt;willReturn(1);

        $testRotation = new ArrayCollection();
        $testRotation-&gt;add([
            'franchise_id' =&gt; 1,
            'ibl' =&gt; 'MAD',
            'rotation' =&gt; 'One, Two, Three'
        ]);

        $viewModel = $this-&gt;createMock(RotationsUsingDoctrine::class);
        $viewModel-&gt;expects($this-&gt;once())
            -&gt;method('getAllByWeek')
            -&gt;willReturn($testRotation);

        $request = $this-&gt;createMock(Request::class);
        $request-&gt;expects($this-&gt;once())
            -&gt;method('getQueryParams')
            -&gt;willReturn(['week' =&gt; 1]);

        // Act
        $query = new RotationManagementQuery($twig, $gamesRepo, $viewModel);
        $results = $query-&gt;__invoke($request);

        // Assert
        self::assertStringContainsString('One, Two, Three', $results-&gt;getBody());
    }
}

</code></pre>

<p>Some thoughts that occur to me from looking at the final test:</p>

<ul>
<li>decoupling makes your dependencies quite visible during test creation</li>
<li>always make sure to only implement the behaviour of your test doubles that you need</li>
<li>your Arrange step will almost always be the largest part of any test</li>
<li>PHPUnit's built-in test double generators also act as assertions</li>
<li>sometimes the simplest way of verifying behaviour is what you should use</li>
</ul>

<p>From my perspective, decoupling the code allows me to focus on smaller
pieces of application behaviour, reducing the chances that a change in
this code breaks something somewhere else.</p>

<p>For more details on the approach I am using for decoupling my code, check
out Matthias Noback's <a href="https://leanpub.com/recipes-for-decoupling">"Recipes for Decoupling"</a>.</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Why isn&#039;t testing ubiquitous?]]></title>
            <link href="https://grumpy-learning.com/blog/2022/11/22/why-isnt-testing-ubiquitous/"/>
            <updated>2022-11-22T00:00:00+00:00</updated>
            <id>https://grumpy-learning.com/blog/2022/11/22/why-isnt-testing-ubiquitous/</id>
            <content type="html"><![CDATA[<p>Over on Twitter Mathias Verraes tweeted something that immediately 
triggered some feelings:</p>

<blockquote>
  <blockquote>
    <p>Perhaps TDD isn't as ubiquitous as it should be because you
    can't make a business model out of it.
    (Original post on Twitter <a href="https://twitter.com/mathiasverraes/status/1595100145129263106">https://twitter.com/mathiasverraes/status/1595100145129263106</a></p>
  </blockquote>
</blockquote>

<p>I commented saying "Boy do I ever have feelings about this topic..." and
Matias asked me to share. I decided my response was better off as
a longer blog post. Don't worry, this will end up on Twitter / Mastodon
anyway.</p>

<p>He mentioned "observability" as a technique that ended up
being a very good business model. Why? In my opinion, figuring out
how to observe something in production is generic enough in that you
can create a tool and say "hey, add these stuff to your code or
production systems, and it will report stuff to this well-crafted
dashboard you can use to get an idea of what is going on."</p>

<p>I am a fan of these approach -- I highly recommend looking into
things like <a href="https://www.honeycomb.io">Honeycomb</a> to get an idea
of what you are signing up for when you choose that path.</p>

<p>So what about testing? Is testing generic enough that you could come
up with some kind of black box or external system that you can connect
your tests to and react to when things fail?</p>

<p>Tests are almost entirely bespoke. Dependent on architecture. Dependent
on environments. Heck, dependant on the skill of the people who have to
write and maintain them.</p>

<p>Also, in my experience, tests work best when you approach them from
the idea that they are there to make sure things are behaving as you
expect them to and to give you a way to determine if you've made changes
that have broken something elsewhere in your application.</p>

<p>In other words, chances are that a failing test is something that the
users of your application will never notice. They will notice broken
pages, non-functioning links, slow-to-respond interfaces. Those are things
that can probably be monitored through observability tools.</p>

<p>In a lot of cases you start off with the idea of having to add tests to
a system being adversarial. Developers don't want to spend the time
writing them. Management views them as tasks with little-to-no return
on investments. Clients balk at being told your bid is more expensive because
you are writing tests. Flaky tests reduce confidence. Build tools need
to be able to play nicely with your chosen testing tools. Effective
test suites can take a lot of time to create and maintain.</p>

<p>Not to mention almost nobody teaches people how to learn to use a programming
language from a test-centric perspective. I could not even imagine
how to teach a novice programmer how to use PHP while also showing them
how to use all the tools. Understanding my own target audience is developers-with-experience
has really changed how I teach and what I teach them.</p>

<p>Using an observability tool
can often be as simple as signing up for an online tool, follow their
directions on what needs to happen to monitor things, and then you
will know a lot faster when things aren't behaving correctly in
production.</p>

<p>Under those types of terms, testing will never be ubiquitous. Which
is a shame because it is a technique that can lead to stable code
bases and confident deployments to production.</p>

<p>Maybe someone out there with a different perspective will figure out
how to solve the stuff I talked about here. Until then, I am still happy
to help teach people how to add automated testing to their skill set
and hope they find it as useful as I have.</p>
]]></content>
        </entry>
            <entry>
            <title type="html"><![CDATA[Asking Companies About Testing]]></title>
            <link href="https://grumpy-learning.com/blog/2022/01/14/asking-companies-about-testing/"/>
            <updated>2022-01-14T00:00:00+00:00</updated>
            <id>https://grumpy-learning.com/blog/2022/01/14/asking-companies-about-testing/</id>
            <content type="html"><![CDATA[<p>This post could also be subtitled "The Grumpy Programmer's Guide to Getting Rejected at Interviews".</p>

<p>Someone tagged me in a tweet...</p>

<blockquote>
  <p><em>Book idea for @grmpyprogrammer: an interviewing guide for job seekers wanting to get an idea of how dedicated companies are to testing. Questions to ask, ways to gauge the culture, etc.</em>
  <em>(Originally posted on Twitter at <a href="https://twitter.com/n00bJackleCity/status/1481632465403981824?s=20">https://twitter.com/n00bJackleCity/status/1481632465403981824?s=20</a>)</em></p>
</blockquote>

<p>...and it got me to thinking about where to start with
a request like this one. My personal opinion that there
really isn't a book in here but it did get me to start thinking
about what sort of questions you should be asking.</p>

<p>Again, keep in mind that all of this is just my opinion. One based
on many years of experience, but still an opinion.</p>

<h2 id="why-does-it-matter%3F">Why Does It Matter?</h2>

<p>In my experience, companies that make a commitment to doing automated
testing also tend to make a commitment towards "quality"
in their coding practices and "automation" in their software development tooling.
The reason those are in quotes is because they definitely can mean 
different things depending on the company.</p>

<p>Now, again, in my experience, you are likely to have more success
in solving problems and growing your own skills as a developer if you work
in an environment where they value those things.</p>

<p>After all, just because we can get paid a lot of money to dig in the pixel
mines doesn't mean we should be forced to eat a shit sandwich. We should at 
least have a choice of the additional toppings.</p>

<h2 id="what-questions-should-i-ask%3F">What Questions Should I Ask?</h2>

<p>Like a lot of things related to programming, I find it helpful to start at the
end result you want and work backwards to figure out what needs to be done. Therefore
I think the first two things to ask are:</p>

<blockquote>
  <blockquote>
    <p>What things always have to work when you push changes into production
    and how do you verify that it works as expected?</p>
  </blockquote>
</blockquote>

<p>This question cuts to the heart of the issue: what matters and how do we make
sure it stays that way.</p>

<p>What you are looking for is clear statements about what matters and clearer statements
about how they verify it. Again, not every company has invested the time and money
into having the ability for code changes to seamlessly flow from a development
environment into production, accompanied by effective automated tests and a clear understanding
of outcomes.</p>

<p>If they already have some kind of commitment to testing, asking follow-up questions
like this are also very informative:</p>

<blockquote>
  <blockquote>
    <p>What do you like about your current testing practices and what do you want to change?</p>
  </blockquote>
</blockquote>

<p>Pay as much attention to what they like as what they dislike. That will give you an idea
of what challenges lie ahead if you want to be the person making the changes.</p>

<p>Finally, if you want to find out about what their commitment to quality is, I feel like
a great question is:</p>

<blockquote>
  <blockquote>
    <p>Tell me about how code gets from the developer and up into production</p>
  </blockquote>
</blockquote>

<p>Look for things like:</p>

<ul>
<li>code reviews</li>
<li>coding standards</li>
<li>static code analysis</li>
<li>continuous integration systems</li>
<li>separate staging and production environments</li>
<li>automated deployments</li>
</ul>

<p>Not all of these things are going to guarantee great results (nothing
does and never believe anyone who says it) but, when taken together,
they show a commitment to making sure that:</p>

<ul>
<li>the intent of code is clear</li>
<li>others can understand the code</li>
<li>the code is taking advantage of appropriate language features</li>
<li>the team uses tooling that integrates with version control to automate error-prone manual checklists</li>
<li>application / end-to-end testing happens before it reaches production</li>
<li>repeatable processes ensure consistency</li>
</ul>

<h2 id="so-now-what%3F">So Now What?</h2>

<p>It's hard for me to give any more specific advice other than "don't be 
afraid to ask more questions based on the answers you are hearing." 
If we're being honest, most companies aren't doing all that stuff I listed
above. You can always start at the bottom ("we try and manually test all changes")
and work as hard as you are allowed to on getting to the point where you
have an automated test suite catching issues before your users do.</p>
]]></content>
        </entry>
    </feed>